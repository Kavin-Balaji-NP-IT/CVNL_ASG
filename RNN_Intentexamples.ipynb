{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Changi Virtual Assist Triage - RNN Text Classification\n",
        "Airport Customer Service Intent Classification System\n",
        "For use in Jupyter Notebook (.ipynb)\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: Install and Import Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "# Uncomment if needed:\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install datasets\n",
        "# !pip install scikit-learn matplotlib seaborn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: Define Changi Airport Intent Categories\n",
        "# ============================================================================\n",
        "\n",
        "# Intent mapping for Changi Virtual Assist\n",
        "CHANGI_INTENTS = {\n",
        "    0: \"flight_status\",\n",
        "    1: \"baggage_issue\",\n",
        "    2: \"terminal_directions\",\n",
        "    3: \"special_assistance\",\n",
        "    4: \"ground_transport\",\n",
        "    5: \"security_customs\"\n",
        "}\n",
        "\n",
        "NUM_CLASSES = len(CHANGI_INTENTS)\n",
        "print(f\"Number of intent categories: {NUM_CLASSES}\")\n",
        "print(\"\\nIntent Categories:\")\n",
        "for idx, intent in CHANGI_INTENTS.items():\n",
        "    print(f\"  {idx}: {intent}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: Create Changi-Specific Dataset\n",
        "# ============================================================================\n",
        "\n",
        "# Sample Changi Airport customer service queries\n",
        "CHANGI_DATA = [\n",
        "    # Flight status queries (0)\n",
        "    (\"What time does flight SQ123 depart?\", 0),\n",
        "    (\"Has the gate changed for my Singapore Airlines flight?\", 0),\n",
        "    (\"Is my flight to Tokyo delayed?\", 0),\n",
        "    (\"When will boarding start for gate C21?\", 0),\n",
        "    (\"Check flight arrival time from London\", 0),\n",
        "    (\"My flight status please\", 0),\n",
        "    (\"What gate is Emirates EK405?\", 0),\n",
        "    (\"Flight departure information needed\", 0),\n",
        "    (\"Show me departures to Bangkok\", 0),\n",
        "    (\"Is SQ308 on time?\", 0),\n",
        "    (\"Gate information for my Qantas flight\", 0),\n",
        "    (\"When does check-in close for BA12?\", 0),\n",
        "    (\"Has my flight started boarding?\", 0),\n",
        "    (\"What's the status of incoming flight from Sydney?\", 0),\n",
        "    (\"Where can I see the departure board?\", 0),\n",
        "\n",
        "    # Baggage issues (1)\n",
        "    (\"My luggage hasn't arrived at the carousel\", 1),\n",
        "    (\"I need to report damaged baggage\", 1),\n",
        "    (\"Where do I claim lost luggage?\", 1),\n",
        "    (\"My suitcase is missing\", 1),\n",
        "    (\"Baggage delayed what should I do\", 1),\n",
        "    (\"Found someone else's bag instead of mine\", 1),\n",
        "    (\"How to file baggage claim\", 1),\n",
        "    (\"Checked bag not on carousel\", 1),\n",
        "    (\"My bag arrived damaged need help\", 1),\n",
        "    (\"Lost baggage counter location\", 1),\n",
        "    (\"Tag says my luggage went to wrong city\", 1),\n",
        "    (\"Bag didn't make connecting flight\", 1),\n",
        "    (\"Need to report missing items from luggage\", 1),\n",
        "    (\"Where is baggage services office?\", 1),\n",
        "    (\"Suitcase handle broken during flight\", 1),\n",
        "\n",
        "    # Terminal directions (2)\n",
        "    (\"How do I get to Jewel from Terminal 2?\", 2),\n",
        "    (\"Where is immigration in Terminal 3?\", 2),\n",
        "    (\"Which way to the transfer desk?\", 2),\n",
        "    (\"Where can I find the SilverKris lounge?\", 2),\n",
        "    (\"Directions to baggage claim area\", 2),\n",
        "    (\"How to walk to Terminal 1 from T3?\", 2),\n",
        "    (\"Where are the charging stations?\", 2),\n",
        "    (\"Location of prayer room please\", 2),\n",
        "    (\"How do I get to the smoking area?\", 2),\n",
        "    (\"Where is the nearest restroom?\", 2),\n",
        "    (\"Directions to Changi Lounge\", 2),\n",
        "    (\"How to reach the butterfly garden?\", 2),\n",
        "    (\"Where can I find currency exchange?\", 2),\n",
        "    (\"Map to duty free shops\", 2),\n",
        "    (\"How to get to departure gates from arrival hall?\", 2),\n",
        "    (\"Where is the medical clinic?\", 2),\n",
        "    (\"Location of children's play area\", 2),\n",
        "\n",
        "    # Special assistance (3)\n",
        "    (\"I need wheelchair assistance\", 3),\n",
        "    (\"Traveling with infant need baby care room\", 3),\n",
        "    (\"Can I get help with mobility?\", 3),\n",
        "    (\"Where to request special assistance?\", 3),\n",
        "    (\"Need help for elderly passenger\", 3),\n",
        "    (\"Stroller rental available?\", 3),\n",
        "    (\"Assistance for visually impaired passenger\", 3),\n",
        "    (\"I'm traveling alone with a toddler need support\", 3),\n",
        "    (\"Require escort service through airport\", 3),\n",
        "    (\"Medical assistance needed\", 3),\n",
        "    (\"Where can I get a wheelchair?\", 3),\n",
        "    (\"Help needed for disabled traveler\", 3),\n",
        "    (\"Nursing room location\", 3),\n",
        "    (\"Do you provide meet and assist service?\", 3),\n",
        "    (\"Special needs passenger support\", 3),\n",
        "\n",
        "    # Ground transport (4)\n",
        "    (\"How do I get to downtown Singapore?\", 4),\n",
        "    (\"Where is the taxi stand?\", 4),\n",
        "    (\"Is there a bus to the city?\", 4),\n",
        "    (\"MRT station location from terminal\", 4),\n",
        "    (\"How much is Grab to Marina Bay?\", 4),\n",
        "    (\"Shuttle bus to nearby hotels\", 4),\n",
        "    (\"Where can I rent a car?\", 4),\n",
        "    (\"Public transport options to Sentosa\", 4),\n",
        "    (\"Taxi queue waiting time\", 4),\n",
        "    (\"How to book airport shuttle?\", 4),\n",
        "    (\"Train to Orchard Road\", 4),\n",
        "    (\"Private car pickup point\", 4),\n",
        "    (\"Bus schedule to city center\", 4),\n",
        "    (\"Cheapest way to get downtown\", 4),\n",
        "    (\"Where do limousines pick up passengers?\", 4),\n",
        "\n",
        "    # Security/Customs (5)\n",
        "    (\"What items can I bring in carry-on?\", 5),\n",
        "    (\"Liquid restrictions for hand luggage\", 5),\n",
        "    (\"Do I need to declare this purchase?\", 5),\n",
        "    (\"Where is customs declaration?\", 5),\n",
        "    (\"Security screening procedures\", 5),\n",
        "    (\"Can I bring duty free alcohol?\", 5),\n",
        "    (\"Immigration form filling help\", 5),\n",
        "    (\"Passport control location\", 5),\n",
        "    (\"What's the customs allowance?\", 5),\n",
        "    (\"Do I go through security again for transfer?\", 5),\n",
        "    (\"Prohibited items list\", 5),\n",
        "    (\"Visa on arrival information\", 5),\n",
        "    (\"Customs declaration for food items\", 5),\n",
        "    (\"How early to arrive for security check?\", 5),\n",
        "    (\"Can I bring batteries in checked luggage?\", 5),\n",
        "]\n",
        "\n",
        "print(f\"Total training samples: {len(CHANGI_DATA)}\")\n",
        "\n",
        "# Show distribution\n",
        "intent_counts = Counter([label for _, label in CHANGI_DATA])\n",
        "print(\"\\nDataset distribution:\")\n",
        "for intent_id, count in sorted(intent_counts.items()):\n",
        "    print(f\"  {CHANGI_INTENTS[intent_id]}: {count} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: Text Preprocessing\n",
        "# ============================================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Handles text cleaning and tokenization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.word_to_idx = self.vocab.copy()\n",
        "        self.idx_to_word = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s?!]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Simple whitespace tokenization\"\"\"\n",
        "        return text.split()\n",
        "\n",
        "    def build_vocab(self, texts, min_freq=1):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        word_freq = Counter()\n",
        "        for text in texts:\n",
        "            cleaned = self.clean_text(text)\n",
        "            tokens = self.tokenize(cleaned)\n",
        "            word_freq.update(tokens)\n",
        "\n",
        "        # Add words to vocabulary\n",
        "        for word, freq in word_freq.items():\n",
        "            if freq >= min_freq and word not in self.word_to_idx:\n",
        "                idx = len(self.word_to_idx)\n",
        "                self.word_to_idx[word] = idx\n",
        "                self.idx_to_word[idx] = word\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
        "        return self.word_to_idx\n",
        "\n",
        "    def text_to_indices(self, text):\n",
        "        \"\"\"Convert text to indices\"\"\"\n",
        "        cleaned = self.clean_text(text)\n",
        "        tokens = self.tokenize(cleaned)\n",
        "        return [self.word_to_idx.get(token, self.word_to_idx['<UNK>'])\n",
        "                for token in tokens]\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Build vocabulary from training data\n",
        "texts = [text for text, _ in CHANGI_DATA]\n",
        "vocab = preprocessor.build_vocab(texts, min_freq=1)\n",
        "\n",
        "# Show sample preprocessing\n",
        "sample_text = \"What time does flight SQ123 depart?\"\n",
        "print(f\"\\nSample preprocessing:\")\n",
        "print(f\"Original: {sample_text}\")\n",
        "print(f\"Cleaned: {preprocessor.clean_text(sample_text)}\")\n",
        "print(f\"Tokens: {preprocessor.tokenize(preprocessor.clean_text(sample_text))}\")\n",
        "print(f\"Indices: {preprocessor.text_to_indices(sample_text)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: Create PyTorch Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class AirportIntentDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for airport intent classification\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, preprocessor):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert text to indices\n",
        "        indices = self.preprocessor.text_to_indices(text)\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'indices': torch.LongTensor(indices),\n",
        "            'label': torch.LongTensor([label]),\n",
        "            'length': len(indices)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for padding sequences\"\"\"\n",
        "    texts = [item['text'] for item in batch]\n",
        "    indices = [item['indices'] for item in batch]\n",
        "    labels = torch.cat([item['label'] for item in batch])\n",
        "    lengths = torch.LongTensor([item['length'] for item in batch])\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_indices = pad_sequence(indices, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'text': texts,\n",
        "        'indices': padded_indices,\n",
        "        'labels': labels,\n",
        "        'lengths': lengths\n",
        "    }\n",
        "\n",
        "# Split data\n",
        "texts, labels = zip(*CHANGI_DATA)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = AirportIntentDataset(X_train, y_train, preprocessor)\n",
        "test_dataset = AirportIntentDataset(X_test, y_test, preprocessor)\n",
        "\n",
        "# Create dataloaders\n",
        "BATCH_SIZE = 8\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Define RNN Model Architecture\n",
        "# ============================================================================\n",
        "\n",
        "class IntentClassifierRNN(nn.Module):\n",
        "    \"\"\"RNN-based intent classifier with LSTM\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
        "                 num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(IntentClassifierRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: (batch_size, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Embedding: (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pack padded sequences\n",
        "        packed = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # Use final hidden state\n",
        "        # hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
        "        if self.bidirectional:\n",
        "            # Concatenate forward and backward hidden states from last layer\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "\n",
        "        # hidden: (batch_size, hidden_dim * num_directions)\n",
        "\n",
        "        # Fully connected layers\n",
        "        out = self.dropout(hidden)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Model hyperparameters\n",
        "VOCAB_SIZE = len(preprocessor.word_to_idx)\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Initialize model\n",
        "model = IntentClassifierRNN(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    bidirectional=BIDIRECTIONAL\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: Define Training Function\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        indices = batch['indices'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        lengths = batch['lengths']\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(indices, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            indices = batch['indices'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            lengths = batch['lengths']\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(indices, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Train the Model\n",
        "# ============================================================================\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "PATIENCE = 10  # For early stopping\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'best_changi_rnn_model.pth')\n",
        "        print(\"  ✓ New best model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_changi_rnn_model.pth'))\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Visualize Training History\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Validation Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: Evaluate on Test Set\n",
        "# ============================================================================\n",
        "\n",
        "# Get predictions\n",
        "_, test_acc, y_pred, y_true = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Classification report\n",
        "intent_names = [CHANGI_INTENTS[i] for i in range(NUM_CLASSES)]\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=intent_names, zero_division=0))\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: Confusion Matrix\n",
        "# ============================================================================\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=intent_names, yticklabels=intent_names)\n",
        "plt.title('Confusion Matrix - Changi Virtual Assist')\n",
        "plt.ylabel('True Intent')\n",
        "plt.xlabel('Predicted Intent')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 12: Inference Function\n",
        "# ============================================================================\n",
        "\n",
        "def predict_intent(model, text, preprocessor, device, top_k=3):\n",
        "    \"\"\"Predict intent for a single text query\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess\n",
        "    indices = preprocessor.text_to_indices(text)\n",
        "    length = len(indices)\n",
        "\n",
        "    # Convert to tensors\n",
        "    indices_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "    length_tensor = torch.LongTensor([length])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(indices_tensor, length_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "\n",
        "    # Get top k predictions\n",
        "    top_probs, top_indices = torch.topk(probabilities, min(top_k, NUM_CLASSES))\n",
        "\n",
        "    results = []\n",
        "    for prob, idx in zip(top_probs, top_indices):\n",
        "        results.append({\n",
        "            'intent': CHANGI_INTENTS[idx.item()],\n",
        "            'confidence': prob.item() * 100\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 13: Test Inference\n",
        "# ============================================================================\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"Where can I find my luggage?\",\n",
        "    \"Is my Emirates flight delayed?\",\n",
        "    \"I need a wheelchair\",\n",
        "    \"How to get to the city center?\",\n",
        "    \"Can I bring this in my carry-on?\",\n",
        "    \"Where is Terminal 2?\",\n",
        "]\n",
        "\n",
        "print(\"Testing intent prediction:\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    predictions = predict_intent(model, query, preprocessor, device, top_k=3)\n",
        "\n",
        "    for i, pred in enumerate(predictions, 1):\n",
        "        print(f\"{i}. {pred['intent']:.<30} {pred['confidence']:>6.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 14: Interactive Prediction\n",
        "# ============================================================================\n",
        "\n",
        "def interactive_demo():\n",
        "    \"\"\"Interactive demo for testing the model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Changi Virtual Assist - Intent Classification Demo\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nEnter passenger queries to classify (type 'quit' to exit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter query: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"\\nThank you for using Changi Virtual Assist!\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        print(\"\\nPredictions:\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        predictions = predict_intent(model, query, preprocessor, device, top_k=3)\n",
        "\n",
        "        for i, pred in enumerate(predictions, 1):\n",
        "            bar = \"█\" * int(pred['confidence'] / 2)\n",
        "            print(f\"{i}. {pred['intent']:.<25} {pred['confidence']:>6.2f}% {bar}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Uncomment to run interactive demo:\n",
        "# interactive_demo()\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 15: Model Analysis and Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nArchitecture: Bidirectional LSTM\")\n",
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
        "print(f\"Hidden Dimension: {HIDDEN_DIM}\")\n",
        "print(f\"Number of Layers: {NUM_LAYERS}\")\n",
        "print(f\"Dropout Rate: {DROPOUT}\")\n",
        "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
        "print(f\"\\nTotal Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTENT CATEGORIES\")\n",
        "print(\"=\"*70)\n",
        "for idx, intent in CHANGI_INTENTS.items():\n",
        "    print(f\"{idx}. {intent}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"USAGE INSTRUCTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "To use this model:\n",
        "\n",
        "1. Preprocess new text:\n",
        "   indices = preprocessor.text_to_indices(\"your query here\")\n",
        "\n",
        "2. Make predictions:\n",
        "   predictions = predict_intent(model, \"your query\", preprocessor, device)\n",
        "\n",
        "3. The model returns top-k intents with confidence scores\n",
        "\n",
        "4. Use the interactive demo for testing:\n",
        "   interactive_demo()\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 16: Save Model and Preprocessor\n",
        "# ============================================================================\n",
        "\n",
        "# Save complete model checkpoint\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab': preprocessor.word_to_idx,\n",
        "    'intents': CHANGI_INTENTS,\n",
        "    'hyperparameters': {\n",
        "        'vocab_size': VOCAB_SIZE,\n",
        "        'embedding_dim': EMBEDDING_DIM,\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT,\n",
        "        'bidirectional': BIDIRECTIONAL\n",
        "    },\n",
        "    'test_accuracy': test_acc\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'changi_rnn_complete.pth')\n",
        "print(\"\\n✓ Model checkpoint saved as 'changi_rnn_complete.pth'\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 17: Load Model (for future use)\n",
        "# ============================================================================\n",
        "\n",
        "def load_changi_model(checkpoint_path):\n",
        "    \"\"\"Load saved model\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Recreate preprocessor\n",
        "    preprocessor_loaded = TextPreprocessor()\n",
        "    preprocessor_loaded.word_to_idx = checkpoint['vocab']\n",
        "    preprocessor_loaded.idx_to_word = {v: k for k, v in checkpoint['vocab'].items()}\n",
        "\n",
        "    # Recreate model\n",
        "    hp = checkpoint['hyperparameters']\n",
        "    model_loaded = IntentClassifierRNN(\n",
        "        vocab_size=hp['vocab_size'],\n",
        "        embedding_dim=hp['embedding_dim'],\n",
        "        hidden_dim=hp['hidden_dim'],\n",
        "        num_classes=hp['num_classes'],\n",
        "        num_layers=hp['num_layers'],\n",
        "        dropout=hp['dropout'],\n",
        "        bidirectional=hp['bidirectional']\n",
        "    )\n",
        "\n",
        "    model_loaded.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model_loaded.to(device)\n",
        "    model_loaded.eval()\n",
        "\n",
        "    return model_loaded, preprocessor_loaded, checkpoint['intents']\n",
        "\n",
        "# Example usage:\n",
        "# model_loaded, preprocessor_loaded, intents = load_changi_model('changi_rnn_complete.pth')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Setup Complete! The model is ready for use.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Sg4dynNeP1Sl",
        "outputId": "251e5887-aa2e-4fd2-e447-5161bc87c1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of intent categories: 6\n",
            "\n",
            "Intent Categories:\n",
            "  0: flight_status\n",
            "  1: baggage_issue\n",
            "  2: terminal_directions\n",
            "  3: special_assistance\n",
            "  4: ground_transport\n",
            "  5: security_customs\n",
            "Total training samples: 92\n",
            "\n",
            "Dataset distribution:\n",
            "  flight_status: 15 samples\n",
            "  baggage_issue: 15 samples\n",
            "  terminal_directions: 17 samples\n",
            "  special_assistance: 15 samples\n",
            "  ground_transport: 15 samples\n",
            "  security_customs: 15 samples\n",
            "Vocabulary size: 258\n",
            "\n",
            "Sample preprocessing:\n",
            "Original: What time does flight SQ123 depart?\n",
            "Cleaned: what time does flight sq123 depart?\n",
            "Tokens: ['what', 'time', 'does', 'flight', 'sq123', 'depart?']\n",
            "Indices: [2, 3, 4, 5, 6, 7]\n",
            "Training samples: 73\n",
            "Test samples: 19\n",
            "Number of training batches: 10\n",
            "Number of test batches: 3\n",
            "IntentClassifierRNN(\n",
            "  (embedding): Embedding(258, 64, padding_idx=0)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 644102\n",
            "Trainable parameters: 644102\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3307305856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;31m# Learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
          ]
        }
      ]
    }
  ]
}